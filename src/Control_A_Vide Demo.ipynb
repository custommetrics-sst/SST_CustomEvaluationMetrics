{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91bcf0fbec754414abe14ad86fd35e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62d223543952463cbb766cd65c832dd9",
              "IPY_MODEL_0b3dbe06733241ca9369f33cc827ad7d",
              "IPY_MODEL_bca4fc66380e48808f7bbe8c6ac24a60",
              "IPY_MODEL_e51539ff05ff4256900b97e681daea25",
              "IPY_MODEL_d187793d66284b838ee2327189a5afff"
            ],
            "layout": "IPY_MODEL_b725864b420b4670b81e33f2fa89514e"
          }
        },
        "62d223543952463cbb766cd65c832dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_553e9d46a5c74a62a7caad22c079bceb",
            "placeholder": "​",
            "style": "IPY_MODEL_1d29a0f3c36d42baa0ab3591ea0bbc7b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0b3dbe06733241ca9369f33cc827ad7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7a63ecb1fd2244f4a6465e51d4fa2325",
            "placeholder": "​",
            "style": "IPY_MODEL_d53301709edb4b33a5a0c104c789af47",
            "value": ""
          }
        },
        "bca4fc66380e48808f7bbe8c6ac24a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_e051de68bec3459383f87a8a5aae59d5",
            "style": "IPY_MODEL_5c6b858bf8c24991ba6d7223353883dd",
            "value": true
          }
        },
        "e51539ff05ff4256900b97e681daea25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_06af859ee30c4d8d8e75d4d2334cb4a7",
            "style": "IPY_MODEL_4f559343df2a4ce9bd6a9a43c2be8feb",
            "tooltip": ""
          }
        },
        "d187793d66284b838ee2327189a5afff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e00bcb946342488fac92787e73d80de2",
            "placeholder": "​",
            "style": "IPY_MODEL_eb24c5a604634a09b3bea9a844c929f6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b725864b420b4670b81e33f2fa89514e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "553e9d46a5c74a62a7caad22c079bceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d29a0f3c36d42baa0ab3591ea0bbc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a63ecb1fd2244f4a6465e51d4fa2325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53301709edb4b33a5a0c104c789af47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e051de68bec3459383f87a8a5aae59d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6b858bf8c24991ba6d7223353883dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06af859ee30c4d8d8e75d4d2334cb4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f559343df2a4ce9bd6a9a43c2be8feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e00bcb946342488fac92787e73d80de2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb24c5a604634a09b3bea9a844c929f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwu7j7uPhqPY",
        "outputId": "f11a877e-64bb-4611-ae94-84eb10f5b6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'control-a-video'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 105 (delta 15), reused 15 (delta 15), pack-reused 89 (from 1)\u001b[K\n",
            "Receiving objects: 100% (105/105), 3.06 MiB | 7.35 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Weifeng-Chen/control-a-video.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r /content/control-a-video/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeyHjjeC4JIm",
        "outputId": "465ae5cb-304a-4592-8226-934aa84846db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers==0.14.0 (from -r /content/control-a-video/requirements.txt (line 1))\n",
            "  Downloading diffusers-0.14.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting transformers==4.27.3 (from -r /content/control-a-video/requirements.txt (line 2))\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl.metadata (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.18.0 (from -r /content/control-a-video/requirements.txt (line 3))\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting xformers==0.0.16 (from -r /content/control-a-video/requirements.txt (line 4))\n",
            "  Downloading xformers-0.0.16-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting imageio==2.27.0 (from -r /content/control-a-video/requirements.txt (line 5))\n",
            "  Downloading imageio-2.27.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting decord==0.6.0 (from -r /content/control-a-video/requirements.txt (line 6))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Collecting opencv-python==4.7.0.72 (from -r /content/control-a-video/requirements.txt (line 7))\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting einops==0.6.0 (from -r /content/control-a-video/requirements.txt (line 8))\n",
            "  Downloading einops-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (0.26.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (11.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.3->-r /content/control-a-video/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.3->-r /content/control-a-video/requirements.txt (line 2)) (6.0.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.3->-r /content/control-a-video/requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.3->-r /content/control-a-video/requirements.txt (line 2)) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Collecting pyre-extensions==0.0.23 (from xformers==0.0.16->-r /content/control-a-video/requirements.txt (line 4))\n",
            "  Downloading pyre_extensions-0.0.23-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting torch>=1.4.0 (from accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.23->xformers==0.0.16->-r /content/control-a-video/requirements.txt (line 4))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.23->xformers==0.0.16->-r /content/control-a-video/requirements.txt (line 4)) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.4.0->accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.4.0->accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.4.0->accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.4.0->accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate==0.18.0->-r /content/control-a-video/requirements.txt (line 3)) (0.45.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.14.0->-r /content/control-a-video/requirements.txt (line 1)) (2024.8.30)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.23->xformers==0.0.16->-r /content/control-a-video/requirements.txt (line 4))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading diffusers-0.14.0-py3-none-any.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.16-cp310-cp310-manylinux2014_x86_64.whl (50.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio-2.27.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tokenizers, opencv-python, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, mypy-extensions, imageio, einops, decord, typing-inspect, nvidia-cudnn-cu11, transformers, torch, pyre-extensions, diffusers, xformers, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.10.0.84\n",
            "    Uninstalling opencv-python-4.10.0.84:\n",
            "      Successfully uninstalled opencv-python-4.10.0.84\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.36.0\n",
            "    Uninstalling imageio-2.36.0:\n",
            "      Successfully uninstalled imageio-2.36.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.0\n",
            "    Uninstalling einops-0.8.0:\n",
            "      Successfully uninstalled einops-0.8.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.2\n",
            "    Uninstalling transformers-4.46.2:\n",
            "      Successfully uninstalled transformers-4.46.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.31.0\n",
            "    Uninstalling diffusers-0.31.0:\n",
            "      Successfully uninstalled diffusers-0.31.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.1.1\n",
            "    Uninstalling accelerate-1.1.1:\n",
            "      Successfully uninstalled accelerate-1.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.2 requires accelerate>=0.21.0, but you have accelerate 0.18.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires imageio>=2.33, but you have imageio 2.27.0 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.27.3 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.18.0 decord-0.6.0 diffusers-0.14.0 einops-0.6.0 imageio-2.27.0 mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opencv-python-4.7.0.72 pyre-extensions-0.0.23 tokenizers-0.13.3 torch-1.13.1 transformers-4.27.3 typing-inspect-0.9.0 xformers-0.0.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub==0.11.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FU14oiR244x",
        "outputId": "5c410786-426a-411c-deb0-f3b2363a290b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub==0.11.1\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.11.1) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.11.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.11.1) (4.66.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.11.1) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.11.1) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.11.1) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.11.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.11.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.11.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.11.1) (2024.8.30)\n",
            "Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/182.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m174.1/182.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.26.2\n",
            "    Uninstalling huggingface-hub-0.26.2:\n",
            "      Successfully uninstalled huggingface-hub-0.26.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.2 requires accelerate>=0.21.0, but you have accelerate 0.18.0 which is incompatible.\n",
            "peft 0.13.2 requires huggingface-hub>=0.17.0, but you have huggingface-hub 0.11.1 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.11.1 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "91bcf0fbec754414abe14ad86fd35e9f",
            "62d223543952463cbb766cd65c832dd9",
            "0b3dbe06733241ca9369f33cc827ad7d",
            "bca4fc66380e48808f7bbe8c6ac24a60",
            "e51539ff05ff4256900b97e681daea25",
            "d187793d66284b838ee2327189a5afff",
            "b725864b420b4670b81e33f2fa89514e",
            "553e9d46a5c74a62a7caad22c079bceb",
            "1d29a0f3c36d42baa0ab3591ea0bbc7b",
            "7a63ecb1fd2244f4a6465e51d4fa2325",
            "d53301709edb4b33a5a0c104c789af47",
            "e051de68bec3459383f87a8a5aae59d5",
            "5c6b858bf8c24991ba6d7223353883dd",
            "06af859ee30c4d8d8e75d4d2334cb4a7",
            "4f559343df2a4ce9bd6a9a43c2be8feb",
            "e00bcb946342488fac92787e73d80de2",
            "eb24c5a604634a09b3bea9a844c929f6"
          ]
        },
        "id": "vYuEUylb4hYe",
        "outputId": "47ad5866-f256-40c7-c18d-4b7b856e29af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y jax jaxlib\n",
        "!pip install jax==0.4.19 jaxlib==0.4.19\n",
        "%cd /content/control-a-video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c7GnGjh5pKF",
        "outputId": "d2295559-f042-4d0a-94f4-ed80b9288d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.4.33\n",
            "Uninstalling jax-0.4.33:\n",
            "  Successfully uninstalled jax-0.4.33\n",
            "Found existing installation: jaxlib 0.4.33\n",
            "Uninstalling jaxlib-0.4.33:\n",
            "  Successfully uninstalled jaxlib-0.4.33\n",
            "Collecting jax==0.4.19\n",
            "  Downloading jax-0.4.19-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting jaxlib==0.4.19\n",
            "  Downloading jaxlib-0.4.19-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.19) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.19) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.19) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.19) (1.13.1)\n",
            "Downloading jax-0.4.19-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.19-cp310-cp310-manylinux2014_x86_64.whl (85.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.87 requires jax>=0.4.27, but you have jax 0.4.19 which is incompatible.\n",
            "chex 0.1.87 requires jaxlib>=0.4.27, but you have jaxlib 0.4.19 which is incompatible.\n",
            "flax 0.8.5 requires jax>=0.4.27, but you have jax 0.4.19 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.19 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.19 which is incompatible.\n",
            "orbax-checkpoint 0.6.4 requires jax>=0.4.26, but you have jax 0.4.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.19 jaxlib-0.4.19\n",
            "/content/control-a-video\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 inference.py --prompt \"a lion walking\" --input_video f\"{path}\" --control_mode canny"
      ],
      "metadata": {
        "id": "vF9Q3YjY4QAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos_prompt_dic = {\n",
        "\n",
        "'A car is moving on an empty road.mp4' :  ['1. a black car is moving on empty road','3. a bus is moving on empty road', '21. a truck is moving on empty road' ] ,\n",
        " 'A clown fish swimming in a coral reef.mp4':  ['6. a dolphin is swimming in a coral reef', '17. a shark is swimming in a coral reef', '20. a tortoise is swimming in a coral reef', '8. a fish is swimming in empty ocean'],\n",
        " 'A duck diving underwater in search of food.mp4': ['7. a dolphin is swimming under water in search of food', '16. a seagull is diving under water insearch of food', '26. an octopus is diving under water in search of food', '27. fish is diving under water in search of food'],\n",
        " 'A rabbit walking on the ground.mp4': ['11. a mouse is walking on the ground', '13. a porcupine is walking on the ground', '14. a rabbit is walking in the forest'],\n",
        " 'A skateboarder performing tricks at a skate park.mp4': ['15. a robot is skateboarding at skate park', '22. a woman is skateboarding at skate park', '24. an alien is skateboarding at skate park'],\n",
        " 'A white cat walking on the grass.mp4': ['2. a black cat is walking on the ground', '10. a lion is walking on the ground', '19. a tiger is walking on the ground', '5. a dog is walking on the ground'],\n",
        " 'A white horse galloping on a street.mp4': ['9. a lion is running on the ground', '23. a zebra is running on the ground', '25. an elephant is running on the ground'],\n",
        " 'a mallard is swimiing in water.mp4': ['4. a crocodile is swimming in the water', '12. a pigeon is swimming in the water', '18. a swan is swimming in the water', '']\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "AG3RBx_v3nTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import time\n",
        "# prompt = \"a white bear walking\"\n",
        "# video = \"bear.mp4\"\n",
        "\n",
        "# start_time = time.time()  # Start timing\n",
        "# command = f'python3 inference.py --prompt \"{prompt}\" --input_video {video} --control_mode canny'\n",
        "# os.system(command)\n",
        "# end_time = time.time()  # End timing\n",
        "# print(\"Time taken:\", end_time - start_time)"
      ],
      "metadata": {
        "id": "isfHpiTIsNGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "for video, prompts in videos_prompt_dic.items():\n",
        "    for prompt in prompts:\n",
        "        command = f'python3 inference.py --prompt \"{prompt}\" --input_video \"{video}\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8'\n",
        "        print(f\"Running command: {command}\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "            )\n",
        "            print(result.stdout.decode())  # Show the script's output\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error occurred while processing video {video} with prompt '{prompt}': {e.stderr.decode()}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Completed all prompts for video: {video}\")"
      ],
      "metadata": {
        "id": "lP8mhSxN0dyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7e855f-a602-485b-e2fd-cd6562f4aa69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running command: python3 inference.py --prompt \"1. a black car is moving on empty road\" --input_video \"A car is moving on an empty road.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"3. a bus is moving on empty road\" --input_video \"A car is moving on an empty road.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"21. a truck is moving on empty road\" --input_video \"A car is moving on an empty road.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A car is moving on an empty road.mp4\n",
            "Running command: python3 inference.py --prompt \"6. a dolphin is swimming in a coral reef\" --input_video \"A clown fish swimming in a coral reef.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"17. a shark is swimming in a coral reef\" --input_video \"A clown fish swimming in a coral reef.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"20. a tortoise is swimming in a coral reef\" --input_video \"A clown fish swimming in a coral reef.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"8. a fish is swimming in empty ocean\" --input_video \"A clown fish swimming in a coral reef.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A clown fish swimming in a coral reef.mp4\n",
            "Running command: python3 inference.py --prompt \"7. a dolphin is swimming under water in search of food\" --input_video \"A duck diving underwater in search of food.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"16. a seagull is diving under water insearch of food\" --input_video \"A duck diving underwater in search of food.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"26. an octopus is diving under water in search of food\" --input_video \"A duck diving underwater in search of food.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"27. fish is diving under water in search of food\" --input_video \"A duck diving underwater in search of food.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A duck diving underwater in search of food.mp4\n",
            "Running command: python3 inference.py --prompt \"11. a mouse is walking on the ground\" --input_video \"A rabbit walking on the ground.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"13. a porcupine is walking on the ground\" --input_video \"A rabbit walking on the ground.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"14. a rabbit is walking in the forest\" --input_video \"A rabbit walking on the ground.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A rabbit walking on the ground.mp4\n",
            "Running command: python3 inference.py --prompt \"15. a robot is skateboarding at skate park\" --input_video \"A skateboarder performing tricks at a skate park.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"22. a woman is skateboarding at skate park\" --input_video \"A skateboarder performing tricks at a skate park.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"24. an alien is skateboarding at skate park\" --input_video \"A skateboarder performing tricks at a skate park.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A skateboarder performing tricks at a skate park.mp4\n",
            "Running command: python3 inference.py --prompt \"2. a black cat is walking on the ground\" --input_video \"A white cat walking on the grass.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"10. a lion is walking on the ground\" --input_video \"A white cat walking on the grass.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"19. a tiger is walking on the ground\" --input_video \"A white cat walking on the grass.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"5. a dog is walking on the ground\" --input_video \"A white cat walking on the grass.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A white cat walking on the grass.mp4\n",
            "Running command: python3 inference.py --prompt \"9. a lion is running on the ground\" --input_video \"A white horse galloping on a street.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"23. a zebra is running on the ground\" --input_video \"A white horse galloping on a street.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"25. an elephant is running on the ground\" --input_video \"A white horse galloping on a street.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Completed all prompts for video: A white horse galloping on a street.mp4\n",
            "Running command: python3 inference.py --prompt \"4. a crocodile is swimming in the water\" --input_video \"a mallard is swimiing in water.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"12. a pigeon is swimming in the water\" --input_video \"a mallard is swimiing in water.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"18. a swan is swimming in the water\" --input_video \"a mallard is swimiing in water.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Running command: python3 inference.py --prompt \"\" --input_video \"a mallard is swimiing in water.mp4\" --control_mode canny --num_sample_frames 16 --each_sample_frame 8\n",
            "Error occurred while processing video a mallard is swimiing in water.mp4 with prompt '': A matching Triton is not available, some optimizations will not be enabled.\n",
            "Error caught was: No module named 'triton'\n",
            "2024-11-29 00:49:10.994414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-29 00:49:11.026919: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-29 00:49:11.037220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-29 00:49:12.839042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "controlnet/diffusion_pytorch_model.safetensors not found\n",
            "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 23172.95it/s]\n",
            "100%|██████████| 20/20 [00:06<00:00,  2.91it/s]\n",
            "100%|██████████| 20/20 [01:54<00:00,  5.70s/it]\n",
            "100%|██████████| 20/20 [01:53<00:00,  5.66s/it]\n",
            "╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n",
            "│ /content/control-a-video/inference.py:141 in <module>                                            │\n",
            "│                                                                                                  │\n",
            "│   138 │   out1 = out1.images[0][1:]    # drop the first frame                                    │\n",
            "│   139 │   out.extend(out1)                                                                       │\n",
            "│   140                                                                                            │\n",
            "│ ❱ 141 imageio.mimsave(f'{prompt}.mp4', out, fps=8)                                               │\n",
            "│   142 # import IPython                                                                           │\n",
            "│   143 # from IPython.display import Image                                                        │\n",
            "│   144 # Image(filename='demo.gif')                                                               │\n",
            "│                                                                                                  │\n",
            "│ /usr/local/lib/python3.10/dist-packages/imageio/v2.py:361 in mimwrite                            │\n",
            "│                                                                                                  │\n",
            "│   358 │                                                                                          │\n",
            "│   359 │   imopen_args = decypher_format_arg(format)                                              │\n",
            "│   360 │   imopen_args[\"legacy_mode\"] = True                                                      │\n",
            "│ ❱ 361 │   with imopen(uri, \"wI\", **imopen_args) as file:                                         │\n",
            "│   362 │   │   return file.write(ims, is_batch=True, **kwargs)                                    │\n",
            "│   363                                                                                            │\n",
            "│   364                                                                                            │\n",
            "│                                                                                                  │\n",
            "│ /usr/local/lib/python3.10/dist-packages/imageio/core/imopen.py:298 in imopen                     │\n",
            "│                                                                                                  │\n",
            "│   295 │   │   │   )                                                                              │\n",
            "│   296 │                                                                                          │\n",
            "│   297 │   request.finish()                                                                       │\n",
            "│ ❱ 298 │   raise err_type(err_msg)                                                                │\n",
            "│   299                                                                                            │\n",
            "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
            "ValueError: Could not find a backend to open `.mp4`` with iomode `wI`.\n",
            "\n",
            "Completed all prompts for video: a mallard is swimiing in water.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nEsYuOPI6yt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}